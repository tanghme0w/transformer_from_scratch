# transformer_from_scratch

Implement a transformer from scratch based on Pytorch.

Reference: [The annotated transformer](https://nlp.seas.harvard.edu/annotated-transformer/#full-model)

## Future plans for this repo
1. Implement encoder-decoder architecture. 
2. Introduce scalability and establish compatibility with other architectures. 
3. Add benchmarking script and make performance improvements.
4. Turn it into a portable python package.